{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kFSqkTCdWKMI"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "awjrpqy-6MaQ"
   },
   "source": [
    "중요: 로컬 머신에서 실행 중인 경우 [설치 지침](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2.md)을 따르세요. 이 노트북에는 Colab에서 실행하는 데 필요한 것만 포함되어 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-vsOL3QR6kqs"
   },
   "source": [
    "저장소의 상위 디렉토리에 `tensorflow/models` 또는 `cd`를 가져옵니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LBdjK2G5ywuc"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "hV4P5gyTWKMI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.22.4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tarfile\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "import cv2\n",
    "import json\n",
    "\n",
    "print(np.__version__)\n",
    "\n",
    "from collections import defaultdict\n",
    "from io import StringIO\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r5FNuiRPWKMN"
   },
   "source": [
    "개체 감지 모듈을 가져옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "4-IMl4b6BdGO"
   },
   "outputs": [],
   "source": [
    "from object_detection.utils import ops as utils_ops\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import visualization_utils as vis_util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RYPCiag2iz_q"
   },
   "source": [
    "패치:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "mF-YlMl8c_bM"
   },
   "outputs": [],
   "source": [
    "# patch tf1 into `utils.ops`\n",
    "utils_ops.tf = tf.compat.v1\n",
    "\n",
    "# Patch the location of gfile\n",
    "tf.gfile = tf.io.gfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cfn_tRFOWKMO"
   },
   "source": [
    "# 모델 준비"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X_sEBLpVWKMQ"
   },
   "source": [
    "## 변수\n",
    "\n",
    "`export_inference_graph.py` 도구를 사용하여 내보낸 모든 모델은 경로를 변경하기만 하면 여기에서 로드할 수 있습니다.\n",
    "\n",
    "기본적으로 여기에서는 \"Mobilenet이 있는 SSD\" 모델을 사용합니다. 외부에서 실행할 수 있는 다른 모델 목록은 [검출 모델 동물원](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md)을 참조하세요. 다양한 속도와 정확도를 가진 상자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ai8pLZZWKMS"
   },
   "source": [
    "## Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "zm8xp-0eoItE"
   },
   "outputs": [],
   "source": [
    "def local_load_model(model_dir):\n",
    "  # base_url = 'http://download.tensorflow.org/models/object_detection/'\n",
    "  # model_file = model_name + '.tar.gz'\n",
    "  # model_dir = tf.keras.utils.get_file(\n",
    "  #   fname=model_name, \n",
    "  #   origin=base_url + model_file,\n",
    "  #   untar=True)\n",
    "\n",
    "  # model_dir = pathlib.Path(model_dir)/\"saved_model\"\n",
    "  model = tf.saved_model.load(str(model_dir))\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_1MVVTcLWKMW"
   },
   "source": [
    "## 라벨 맵 로드 중\n",
    "레이블은 인덱스를 범주 이름에 매핑하므로 컨볼루션 네트워크가 '5'를 예측할 때 이것이 '비행기'에 해당한다는 것을 알 수 있습니다. 여기서 내부 유틸리티 함수를 사용하지만 정수를 적절한 문자열 레이블에 매핑하는 사전을 반환하는 모든 것이 좋습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "hDbpHkiWWKMX"
   },
   "outputs": [],
   "source": [
    "# List of the strings that is used to add correct label for each box.\n",
    "PATH_TO_LABELS = 'annotations/label_map.pbtxt'\n",
    "category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS, use_display_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oVU3U_J6IJVb"
   },
   "source": [
    "간단하게 하기 위해 2개의 이미지에 대해 테스트합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "jG-zn5ykWKMd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WindowsPath('test_images/1111.jpg'),\n",
       " WindowsPath('test_images/13_293_GA1_평면도 기본형_412_13974_13_293_GA1_1292393292088.jpg'),\n",
       " WindowsPath('test_images/2_113B_GA1_평면도 기본형_GW70711_1334651472207.jpg'),\n",
       " WindowsPath('test_images/2_150_GA1_평면도 기본형_GW70711_1334647273143.jpg'),\n",
       " WindowsPath('test_images/2_152_GA1_평면도 기본형_photoinfra_1623198571628.jpg'),\n",
       " WindowsPath('test_images/2_159_GA1_평면도 기본형_GW70711_1330414332294.jpg'),\n",
       " WindowsPath('test_images/3333.jpg'),\n",
       " WindowsPath('test_images/3_142_GA1_평면도 기본형_GW70711_1332231018655.jpg'),\n",
       " WindowsPath('test_images/3_158_GA1_평면도 기본형_412_2601_3_155_1271381461910.jpg'),\n",
       " WindowsPath('test_images/3_190A_GA1_평면도 기본형_GW70711_1330413835572.jpg'),\n",
       " WindowsPath('test_images/4444.jpg'),\n",
       " WindowsPath('test_images/5_258_GA1_평면도 기본형_116_18522_5_257_GA1_1280128725168.jpg'),\n",
       " WindowsPath('test_images/7_126_GA1_평면도 기본형_GW70711_1331110871245.jpg'),\n",
       " WindowsPath('test_images/7_153_GA1_평면도 기본형_GW70711_1330074672164.jpg'),\n",
       " WindowsPath('test_images/7_269_GA1_평면도 기본형_GW70711_1332232051759.jpg'),\n",
       " WindowsPath('test_images/9_102_GA1_평면도 기본형_GW70711_1332233220919.jpg'),\n",
       " WindowsPath('test_images/9_281_GA1_평면도 기본형_412_12418_9_280_1276741870746.jpg'),\n",
       " WindowsPath('test_images/9_349A_GA1_평면도 기본형_GW70711_1330412047905.jpg')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you want to test the code with your images, just add path to the images to the TEST_IMAGE_PATHS.\n",
    "import pathlib\n",
    "\n",
    "PATH_TO_TEST_IMAGES_DIR = pathlib.Path('test_images')\n",
    "TEST_IMAGE_PATHS = sorted(list(PATH_TO_TEST_IMAGES_DIR.glob(\"*.jpg\")))\n",
    "TEST_IMAGE_PATHS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H0_1AGhrWKMc"
   },
   "source": [
    "# 탐지"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f7aOtOlebK7h"
   },
   "source": [
    "객체 감지 모델 로드:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "1XNT0wxybKR6"
   },
   "outputs": [],
   "source": [
    "model_dir = \"exported-models/my_faster_rcnn_model/saved_model/\"\n",
    "detection_model = local_load_model(model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yN1AYfAEJIGp"
   },
   "source": [
    "모델의 입력 서명을 확인하면 uint8 유형의 3색 이미지 배치가 필요합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "oLSZpfaYwuSk"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'detection_boxes': tf.float32,\n",
       " 'detection_multiclass_scores': tf.float32,\n",
       " 'raw_detection_boxes': tf.float32,\n",
       " 'raw_detection_scores': tf.float32,\n",
       " 'detection_classes': tf.float32,\n",
       " 'num_detections': tf.float32,\n",
       " 'detection_anchor_indices': tf.float32,\n",
       " 'detection_scores': tf.float32}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detection_model.signatures['serving_default'].output_dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "FZyKUJeuxvpT"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'detection_boxes': TensorShape([1, 300, 4]),\n",
       " 'detection_multiclass_scores': TensorShape([1, 300, 87]),\n",
       " 'raw_detection_boxes': TensorShape([1, 300, 4]),\n",
       " 'raw_detection_scores': TensorShape([1, 300, 87]),\n",
       " 'detection_classes': TensorShape([1, 300]),\n",
       " 'num_detections': TensorShape([1]),\n",
       " 'detection_anchor_indices': TensorShape([1, 300]),\n",
       " 'detection_scores': TensorShape([1, 300])}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detection_model.signatures['serving_default'].output_shapes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JP5qZ7sXJpwG"
   },
   "source": [
    "Add a wrapper function to call the model, and cleanup the outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ajmR_exWyN76"
   },
   "outputs": [],
   "source": [
    "def run_inference_for_single_image(model, image):\n",
    "  image = np.asarray(image)\n",
    "  # 입력은 텐서여야 하며 `tf.convert_to_tensor`를 사용하여 변환합니다.\n",
    "  input_tensor = tf.convert_to_tensor(image)\n",
    "  # 모델은 이미지 배치를 예상하므로 `tf.newaxis`를 사용하여 축을 추가합니다.\n",
    "  input_tensor = input_tensor[tf.newaxis,...]\n",
    "\n",
    "  # 추론 (inference) 실행\n",
    "  model_fn = model.signatures['serving_default']\n",
    "  output_dict = model_fn(input_tensor)\n",
    "\n",
    "  # 모든 출력은 배치 텐서입니다.\n",
    "  # numpy 배열로 변환하고 인덱스 [0]을 사용하여 배치 차원을 제거합니다.\n",
    "  # 우리는 첫 번째 num_detections에만 관심이 있습니다.\n",
    "  num_detections = int(output_dict.pop('num_detections'))\n",
    "  output_dict = {key:value[0, :num_detections].numpy() \n",
    "                 for key,value in output_dict.items()}\n",
    "  output_dict['num_detections'] = num_detections\n",
    "\n",
    "  # detection_classes는 정수여야 합니다.\n",
    "  output_dict['detection_classes'] = output_dict['detection_classes'].astype(np.int64)\n",
    "   \n",
    "  # 마스크가 있는 모델 처리:\n",
    "  if 'detection_masks' in output_dict:\n",
    "    # bbox 마스크를 이미지 크기로 재구성합니다.\n",
    "    detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
    "              output_dict['detection_masks'], output_dict['detection_boxes'],\n",
    "               image.shape[0], image.shape[1])      \n",
    "    detection_masks_reframed = tf.cast(detection_masks_reframed > 0.5, tf.uint8)\n",
    "    output_dict['detection_masks_reframed'] = detection_masks_reframed.numpy()\n",
    "\n",
    "  return output_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JP5qZ7sXJpwG"
   },
   "source": [
    "이미지 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imread(filename, flags=cv2.IMREAD_COLOR, dtype=np.uint8):\n",
    "    try:\n",
    "        n = np.fromfile(filename, dtype)\n",
    "        img = cv2.imdecode(n, flags)\n",
    "        return img\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z1wq0LVyMRR_"
   },
   "source": [
    "각 테스트 이미지에서 Bounding box와 line을 json 형태로 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "DWh_1zz6aqxs"
   },
   "outputs": [],
   "source": [
    "def export(model, image_path):\n",
    "  # 이미지의 배열 기반 표현은 나중에 준비하기 위해 사용됩니다. 상자와 레이블이 있는 결과 이미지입니다.\n",
    "  image_np = np.array(Image.open(image_path))\n",
    "  # 실제 감지.\n",
    "  output_dict = run_inference_for_single_image(model, image_np)\n",
    "\n",
    "  height, width, _ = image_np.shape\n",
    "  obj_index = output_dict['detection_scores'] > 0.5\n",
    "  scores = output_dict['detection_scores'][obj_index]\n",
    "  boxes = output_dict['detection_boxes'][obj_index]\n",
    "  classes = output_dict['detection_classes'][obj_index]\n",
    "\n",
    "  data = {}\n",
    "  data['detection_boxes'] = []\n",
    "\n",
    "  for box, cls, score in zip(boxes, classes, scores):\n",
    "    data['detection_boxes'].append({\n",
    "      \"name\" : category_index[cls]['name'],\n",
    "      \"min\" : [int(box[1] * width), int(box[0] * height)],\n",
    "      \"max\" : [int(box[3] * width), int(box[2] * height)]\n",
    "    })\n",
    "\n",
    "  with open(\"./\" + image_path.stem + \".json\", 'w') as outfile:\n",
    "    json.dump(data, outfile, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "3a5wMHN8WKMh"
   },
   "outputs": [],
   "source": [
    "# for image_path in TEST_IMAGE_PATHS:\n",
    "#   export(detection_model, image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export(model, image_path):\n",
    "\n",
    "    from xml.etree.ElementTree import Element, SubElement, ElementTree\n",
    "\n",
    "    def _pretty_print(current, parent=None, index=-1, depth=0):\n",
    "        for i, node in enumerate(current):\n",
    "            _pretty_print(node, current, i, depth + 1)\n",
    "        if parent is not None:\n",
    "            if index == 0:\n",
    "                parent.text = '\\n' + ('\\t' * depth)\n",
    "            else:\n",
    "                parent[index - 1].tail = '\\n' + ('\\t' * depth)\n",
    "            if index == len(parent) - 1:\n",
    "                current.tail = '\\n' + ('\\t' * (depth - 1))\n",
    "                \n",
    "    image_np = np.array(Image.open(image_path))\n",
    "    # 실제 감지.\n",
    "    output_dict = run_inference_for_single_image(model, image_np)\n",
    "\n",
    "    height, width, _ = image_np.shape\n",
    "    obj_index = output_dict['detection_scores'] > 0.5\n",
    "    scores = output_dict['detection_scores'][obj_index]\n",
    "    boxes = output_dict['detection_boxes'][obj_index]\n",
    "    classes = output_dict['detection_classes'][obj_index]\n",
    "\n",
    "\n",
    "    root = Element(\"annotation\")\n",
    "\n",
    "    element1 = Element(\"folder\")\n",
    "    element1.text = \"labelledImg_folder\"\n",
    "    root.append(element1)\n",
    "\n",
    "    element2 = Element(\"filename\")\n",
    "    element2.text = \"file_name_test1.jpg\"\n",
    "    root.append(element2)\n",
    "\n",
    "    element3 = Element(\"path\")\n",
    "    element3.text = \"PATH_TO_TEST_IMAGES_DIR, +.jpg\"\n",
    "    root.append(element3)\n",
    "\n",
    "    element4 = Element(\"source\")\n",
    "    root.append(element4)\n",
    "\n",
    "    sub_element4_1 = SubElement(element4, \"database\")\n",
    "    sub_element4_1.text = \"Unknown\"\n",
    "\n",
    "    element5 = Element(\"size\")\n",
    "    root.append(element5)\n",
    "    sub_element5_1 = SubElement(element5, \"width\")\n",
    "    sub_element5_1.text = \"923\"\n",
    "    sub_element5_2 = SubElement(element5, \"height\")\n",
    "    sub_element5_2.text = \"676\"\n",
    "    sub_element5_3 = SubElement(element5, \"depth\")\n",
    "    sub_element5_3.text = \"3\"\n",
    "\n",
    "    element6 = Element(\"segmented\")\n",
    "    element6.text = \"0\"\n",
    "    root.append(element6)\n",
    "\n",
    "    #-------------object( = x,y__min,max start)\n",
    "    \n",
    "    for box, cls, score in zip(boxes, classes, scores):\n",
    "        \n",
    "\n",
    "        count = 7\n",
    "\n",
    "        globals()['element{}'.format(count)] = Element(\"object\")\n",
    "        root.append(element7)\n",
    "        sub_element7_1 = SubElement(element7, \"name\")\n",
    "        sub_element7_1.text = category_index[cls]['name']\n",
    "        sub_element7_2 = SubElement(element7, \"pose\")\n",
    "        sub_element7_2.text = \"Unspecified\"\n",
    "        sub_element7_3 = SubElement(element7, \"truncated\")\n",
    "        sub_element7_3.text = \"0\"\n",
    "        sub_element7_4 = SubElement(element7, \"difficult\")\n",
    "        sub_element7_4.text = \"0\"\n",
    "        sub_element7_5 = SubElement(element7, \"bndbox\")\n",
    "        sub_element7_5_1 = SubElement(sub_element7_5, \"xmin\")\n",
    "        sub_element7_5_1.text = int(box[1] * width)\n",
    "        sub_element7_5_2 = SubElement(sub_element7_5, \"ymin\")\n",
    "        sub_element7_5_2.text = int(box[0] * height)\n",
    "        sub_element7_5_3 = SubElement(sub_element7_5, \"xmax\")\n",
    "        sub_element7_5_3.text = int(box[3] * width)\n",
    "        sub_element7_5_4 = SubElement(sub_element7_5, \"ymax\")\n",
    "        sub_element7_5_4.text = int(box[2] * height)\n",
    "        \n",
    "        count = count + 1\n",
    "    #-------------object end;\n",
    "\n",
    "    _pretty_print(root)\n",
    "\n",
    "    tree = ElementTree(root)\n",
    "\n",
    "    fileName = \"example.xml\"\n",
    "\n",
    "    with open(\"./\" + image_path.stem + \".xml\", \"wb\") as file:\n",
    "        tree.write(file, encoding='utf-8', xml_declaration=True)\n",
    "        \n",
    "    # with open(\"./\" + image_path.stem + \".xml\", 'w') as outfile:\n",
    "    #     json.dump(data, outfile, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_path in TEST_IMAGE_PATHS:\n",
    "  export(detection_model, image_path)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "last_runtime": {
    "build_target": "//learning/brain/python/client:colab_notebook",
    "kind": "private"
   },
   "name": "object_detection_tutorial.ipynb",
   "private_outputs": true,
   "provenance": [
    {
     "file_id": "/piper/depot/google3/third_party/tensorflow_models/object_detection/colab_tutorials/object_detection_tutorial.ipynb",
     "timestamp": 1594335690840
    },
    {
     "file_id": "1LNYL6Zsn9Xlil2CVNOTsgDZQSBKeOjCh",
     "timestamp": 1566498233247
    },
    {
     "file_id": "/piper/depot/google3/third_party/tensorflow_models/object_detection/object_detection_tutorial.ipynb?workspaceId=markdaoust:copybara_AFABFE845DCD573AD3D43A6BAFBE77D4_0::citc",
     "timestamp": 1566488313397
    },
    {
     "file_id": "/piper/depot/google3/third_party/py/tensorflow_docs/g3doc/en/r2/tutorials/generative/object_detection_tutorial.ipynb?workspaceId=markdaoust:copybara_AFABFE845DCD573AD3D43A6BAFBE77D4_0::citc",
     "timestamp": 1566145894046
    },
    {
     "file_id": "1nBPoWynOV0auSIy40eQcBIk9C6YRSkI8",
     "timestamp": 1566145841085
    },
    {
     "file_id": "/piper/depot/google3/third_party/tensorflow_models/object_detection/object_detection_tutorial.ipynb?workspaceId=markdaoust:copybara_AFABFE845DCD573AD3D43A6BAFBE77D4_0::citc",
     "timestamp": 1556295408037
    },
    {
     "file_id": "1layerger-51XwWOwYMY_5zHaCavCeQkO",
     "timestamp": 1556214267924
    },
    {
     "file_id": "/piper/depot/google3/third_party/tensorflow_models/object_detection/object_detection_tutorial.ipynb?workspaceId=markdaoust:copybara_AFABFE845DCD573AD3D43A6BAFBE77D4_0::citc",
     "timestamp": 1556207836484
    },
    {
     "file_id": "1w6mqQiNV3liPIX70NOgitOlDF1_4sRMw",
     "timestamp": 1556154824101
    },
    {
     "file_id": "https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb",
     "timestamp": 1556150293326
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "6fff98fc3b3d81bd655c2cc48858186e4d9e2db7b515bf1c3221888f12a62f87"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
